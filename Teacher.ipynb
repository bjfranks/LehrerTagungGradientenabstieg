{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58771424-79d4-4d25-9191-15520b1ce565",
   "metadata": {},
   "source": [
    "Die folgende Zelle müsst ihr einmal ausführen. Also konkret auf die Zelle klicken und dann auf den Playbutton über diesem Text klicken."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb457bb-faa8-4d80-9e0a-3cb85c429d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c59aa7b2-2e58-4023-8555-c200d983e30e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Python Basics\n",
    "Im Folgenden sind alle notwendigen grundlegenden Python-Instruktionen gegeben."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0440ae31-5cd9-4bee-ac8e-aa33a7a88319",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 0\n",
    "b = a+3\n",
    "c = b*2\n",
    "d = c/3\n",
    "print(a)\n",
    "print(b)\n",
    "print(a,b,c,d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12201911-7b5a-4f50-9890-d88b2aad0e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [5, 3, 6]\n",
    "print(a, a[0], a[1], a[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e1d5a1c-2dbc-4934-8b7f-cb74c5e1643b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(20): # range(20) erzeugt quasi eine Liste (iterator) von indices von 0 bis (20-1) über die dann iteriert wird. list(range(20)) gibt euch die wahre Liste\n",
    "    print(i, end=\", \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c566aa-e50e-45e5-a0dd-3b61df88f4fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in a: # a wurde zuvor als a = [5, 3, 6] definiert\n",
    "    print(i, end=\", \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f949dae5-4157-49dc-b5f9-0d88d2525991",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"{a}\") #String\n",
    "print(f\"{a}\") #String indem die variable a interpretiert wird"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af5379b6-09cc-4828-b68a-1d51a137526f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def foo(a): #foo ist der Name der Funktion, a ist der parameter\n",
    "    print(f\"Hello {a}\")\n",
    "\n",
    "foo(\"World\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90bc67fc-9976-4cad-8ed8-fe771c90db9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Foo():\n",
    "    def __init__(self, inp): # Initialisierung, Aufruf Foo(inp)\n",
    "        self.inp = inp\n",
    "\n",
    "    def __call__(self): # Aufruf der Klasse als wäre sie eine Funktion, Auruf Foo(inp)()\n",
    "        return self.inp\n",
    "\n",
    "    def foo(self): # Aufruf einer definierten Methode auf der Klasse, Aufruf Foo(inp).foo()\n",
    "        return self.inp + self.inp\n",
    "\n",
    "object = Foo(\"input\") # input ist ein String\n",
    "call = object()\n",
    "foo = object.foo()\n",
    "print(object, call, foo)\n",
    "\n",
    "object = Foo(7) # input ist die Zahl 7\n",
    "call = object()\n",
    "foo = object.foo()\n",
    "print(object, call, foo)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c606887-4c53-4449-8d38-38611f49cd3a",
   "metadata": {},
   "source": [
    "## Numpy\n",
    "Im Folgenden findet Ihr Instruktionen aus dem NumPy-Paket, welches wir als np importiert haben. Dieses Paket vereinfacht das Rechnen mit Vektoren und Matrizen.\n",
    "### Inneres Produkt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46cf6e84-50da-43cd-9bce-6b65ec6e7c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([1,2,3]) # np.array(a) convertiert eine Liste a in einen Vektor der die Elemente aus a enthält.\n",
    "b = np.array([1,1,1])\n",
    "print(a,b,np.inner(a, b)) # Inneres Produkt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f9b38e-65a4-49f1-b663-70c28bd9962b",
   "metadata": {},
   "source": [
    "### Matrix Multiplikation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58dad8bf-1309-4d48-a6f9-9ac4ff1b9984",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[1,2],[3,4]]) # np.array kann auch mit Listen von Listen arbeiten und konvertiert diese dann zu Matrizen (oder Tensoren bei Listen von Listen von ...)\n",
    "w = np.array([1,1])\n",
    "print(np.matmul(X,w)) # [1+2 3+4] = [3 7]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b43b68-7d79-4b2f-a654-63b80ae4ca6c",
   "metadata": {},
   "source": [
    "### Transponieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8574aa3-d93e-4917-b7cb-fb9ba1ca509e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X)\n",
    "print(X.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c49f746-6827-407a-8a76-741d91eb015c",
   "metadata": {},
   "source": [
    "### Euklidische Norm / Vektorlänge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c3111c-70d7-4081-a54a-caf319a4a140",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.linalg.norm(np.array([1, 1, 1, 1]))) # \\qrt{1^2+1^2+1^2+1^2} = \\sqrt{4} = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d155e3-226a-443c-bac3-dd7834ef32ba",
   "metadata": {},
   "source": [
    "### Zufällige Vektoren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef4617d-5075-47b9-b546-5a44ee88551b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.random.rand(5)) # np.random.rand(a) erzeugt einen vektor mit 5 zufälligen Zahlen zwischen 0 und 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d7899e-baba-4953-90a1-8a0141aa1d51",
   "metadata": {},
   "source": [
    "# 1. Lineare Regression ohne y-Achsenabschnitt\n",
    "Erst erstellen wir einen Datensatz. Dann versuchen wir von Hand die Funktion $g(x)=ax$ an unsere Datenanzupassen indem wir mit $a$ herumspielen. Danach implementieren wir das Gradientenverfahren sowie die Least Squares Regression, welche dann automatisch $a$ für uns findet."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05018c75-f2ca-44bb-97ab-cd0a3405c4e6",
   "metadata": {},
   "source": [
    "## Linearen Datensatz erstellen\n",
    "Wir erstellen einen Linearen Datensatz mit etwas Messfehlern um das problem etwas Interessanter zu machen. Hier gilt es nichts zu tun, außer die Zelle auszuführen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27c244b9-71fd-4500-8fcd-80115987974f",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_points = 100\n",
    "rand_scale = 100\n",
    "_a = 13.78\n",
    "x_0 = np.array([i-50 for i in range(num_points)])\n",
    "x = x_0\n",
    "y = _a*x_0+rand_scale*np.random.rand(num_points)-rand_scale/2\n",
    "\n",
    "plt.scatter(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8136bbbf-97b5-4c38-ba4b-8b388eff32d3",
   "metadata": {},
   "source": [
    "## Von Hand\n",
    "Zuerst können wir versuchen, die Steigung von Hand zu berechnen. Falls ihr den vorherigen Code versteht, könnt ihr dort natürlich die korrekte Steigung ablesen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c46eacd-951a-4462-94fc-5379731a8a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO Verändert dieses a damit die Linie gut auf den Daten liegt.\n",
    "a = 0\n",
    "\n",
    "plt.scatter(x, y)\n",
    "plt.plot(x, a*x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5491ed00-8630-4371-b1bf-dc7941b458fd",
   "metadata": {},
   "source": [
    "## Gradientenverfahren implementieren\n",
    "Im Folgenden implementieren wir das Gradientenverfahren. $f$ ist die Funktion, die wir minimieren möchten. Wir nehmen an, dass $f$.gradient($p$) uns die Ableitung an der Stelle $p$ liefert. Diese definieren wir als Nächstes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a992f3-be92-4ccf-b495-bed61a067b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradientenverfahren(f, p_0, T, lr):\n",
    "    print(f\"Starting gradient descent with parameter p_0={p_0}, T={T}, and learning rate lr={lr}.\")\n",
    "    p = p_0\n",
    "    # TODO Berechne p T mal iterativ neu über den Gradientenabstieg, von f, mit Lernrate lr\n",
    "    \n",
    "    print(f\"Ended gradient descent with parameter p={p} and a gradient of {f.gradient(p)}.\")\n",
    "    return p"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d8e7c84-98fc-4fc7-a2c3-003d0c3b9438",
   "metadata": {},
   "source": [
    "## Lineare Regression definieren\n",
    "Jetzt definieren wir die Funktion, welche es zu minimieren gilt. Konkret handelt es sich hierbei um $f(a) = ||g(x)-y||^2 = ||ax-y||^2$, wobei wir hier nach $a$ und nicht nach $x$ optimieren und $x$ sowie $y$ vectoren voller Datenpunkte sind. Also es gilt für das optimale $g$ dass $\\forall i : g(x_i)=y_i$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deed8f1a-991c-453c-adce-57624511b1de",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear_Regression():\n",
    "    def __init__(self, x, y): # Hier speichern wir uns die Daten. Später kann man auf sie zurückgreifen mit self.x und self.y\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "    \n",
    "    def __call__(self, a):\n",
    "        # TODO berechne die Vorhersage, also ax. Diese Funktion wird für die Zeichnungen verwendet.\n",
    "        return 0*self.x\n",
    "\n",
    "    def gradient(self, a):\n",
    "        # TODO berechne den Gradienten des quadratischen Fehlers in Bezug auf a. Also konkreter ||ax-y||^2=a^2||x||^2+||y||^2-2a<x,y>.\n",
    "        # Die Ableitung nach a ist dann 2a||x||^2-2<x,y>\n",
    "        result = 0\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1714fb2-e7cb-4869-9f4e-6bb9aa8ac727",
   "metadata": {},
   "source": [
    "## Lineare Regression per Gradientenverfahren trainieren\n",
    "Jetzt müssen wir nur die folgende Zelle ausführen und alles \"sollte klappen\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcfb007a-aabb-4610-83eb-47517195e0f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Linear_Regression(x,y)\n",
    "a_sol = gradientenverfahren(f=model, p_0=0, T=10, lr=0.01)\n",
    "plt.scatter(x, y)\n",
    "plt.plot(x, model(a_sol))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e6fa25-884f-4faf-80ef-c586a2733913",
   "metadata": {},
   "source": [
    "Vermutlich scheitert das Gradientenverfahren, weil wir zu wenige Schritte machen, also $T$ zu klein ist, und/oder weil die Lernrate $lr$ zu groß oder zu klein gewählt ist. Versuche mal $lr$ zu verringern und $T$ zu erhöhen bis die Optimierung nicht mehr divergiert."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16ade686-680a-4163-8456-a92b5c64f70d",
   "metadata": {},
   "source": [
    "# 2. Lineare Regression mit y-Achsenabschnitt\n",
    "Wir verwenden denselben Datensatz und den bisherigen Code, passen jedoch unsere Klasse für lineare Regression an. Im Machinellen lernen benutzt man für den y-Achsenabschnitt den Term bias, welcher im folgenden einige male auftreten wird."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2420a12b-2d82-46ed-9a07-2a4c2c14ef5d",
   "metadata": {},
   "source": [
    "Zuerst passen wir die Daten an, damit das Problem interessanter wird:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca66e96-9b46-4034-aff7-2f4f7a147287",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = 53\n",
    "y = _a*x_0+b+rand_scale*np.random.rand(num_points)-rand_scale/2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fae803e-9f9f-4783-a7d0-baded1b31dd3",
   "metadata": {},
   "source": [
    "Stelle sicher, dass im folgenden Code deine Wahl von $T$ und $lr$ übernommen ist. Jetzt stellen wir sicher, dass unser vorheriges Modell nicht funktioniert. Führe einfach den folgenden code aus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe2a7206-4f97-45c9-9b91-a4b7a0dfb027",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Linear_Regression(x,y)\n",
    "a_sol = gradientenverfahren(f=model, p_0=0, T=10, lr=0.01)\n",
    "plt.scatter(x, y)\n",
    "plt.plot(x, model(a_sol))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "612c0260-6447-4c58-8416-0bb3b00a87b2",
   "metadata": {},
   "source": [
    "Man sollte jetzt sehen, dass das Modell leicht daneben liegt. Dies liegt daran, dass wir bis jetzt keinen $b$-Term ($g(x)=ax+b$) hatten. Die Funktion kann die Daten deshalb nicht perfekt abbilden. Also fügen wir jetzt diesen $b$-Term hinzu.\n",
    "\n",
    "Dafür definieren wir die neue Klasse:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d3f5958-d30d-4198-a7e0-f73a00b17f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear_Regression_Bias():\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "    \n",
    "    def __call__(self, p):\n",
    "        # TODO Wir nehmen an, dass a und b in p verpackt sind. Konkret a = p[0] und b=p[1].\n",
    "        # Berechne die Vorhersage, also ax+b. Diese Funktion wird für die Zeichnungen verwendet. \n",
    "        return 0*self.x\n",
    "\n",
    "    def gradient(self, p):\n",
    "        # TODO berechne den gradienten des quadratischen Fehlers in Bezug auf a und b. \n",
    "        # Wie zuvor a=p[0], b=p[1]. Konkreter, wo die 1en ein vektor voller 1 darstellt (np.ones(len(x))) und n die Anzahl an Datenpunkten ist (len(x)):\n",
    "        # ||ax+1b-y||^2=a^2||x||^2+||y||^2+nb^2+2ab<1,x>-2a<x,y>-2b<1,y>.\n",
    "        # Die Ableitung nach a ist dann 2a||x||^2-2<x,y>+2b<1,x> = 2(a||x||^2-<x,y>+b<1,x>)\n",
    "        # Die Ableitung nach b ist dann 2a<1,x>-2<1,y>+2nb = 2(a<1,x>-<1,y>+nb)\n",
    "        # Diese Funktion sollte ein np.array, mit der Ableitung nach a an der ersten und der Ableitung nach b and der zweiten Stelle, zurückliefern.\n",
    "        return np.array([0,\n",
    "                         0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "677dc5da-e20d-4489-a358-080adcf4fc3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Linear_Regression_Bias(x,y)\n",
    "p_sol = gradientenverfahren(f=Linear_Regression_Bias(x,y), p_0=np.array([0,0]), T=10, lr=0.01)\n",
    "plt.scatter(x, y)\n",
    "plt.plot(x, model(p_sol))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5783c9e-14ea-4a61-8960-59ae996e5825",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 3. Nicht-Lineare Regression\n",
    "Im Folgenden betrachten wir Datensätze, die nicht linear approximierbar sind. Man kann beliebige, auch nichtlineare, Funktionen betrachten, solange sie ableitbar sind. Je nachdem wieviel zeit ihr noch habt würde ich empfehlen, dass ihr hier nichts lösst sondern euch nur vor Augen führt was es hier zu tun gilt.\n",
    "\n",
    "Erinnert euch, dass es hier darum geht dass wir $f(p) = ||g_p(x)-y||^2$ optimieren. Also $g$ hängt von den Parametern $p$ ab und wir versuchen $p$ zu finden. Im folgenden nehmen wir an, dass $g(x)=ax^2+bx+c$ oder $g(x)=a^x$. Es gilt diesmal aber alles selbst zu machen, also Ableitungen müsst ihr selbst berechnen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "423031e2-260a-441a-9f88-6b988a2a0a5f",
   "metadata": {},
   "source": [
    "## Quadratisch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce30537-6824-490a-a96f-23b7e75f2ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_points = 100\n",
    "rand_scale = 300\n",
    "a = 1\n",
    "b = 0.5\n",
    "c = 2\n",
    "x_0 = np.array([i-50 for i in range(num_points)])\n",
    "x = x_0\n",
    "y = a*x_0**2+b*x_0+c+rand_scale*np.random.rand(num_points)-rand_scale/2\n",
    "\n",
    "plt.scatter(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec4c0637-d84a-4ee6-adcd-1f2127fdbce2",
   "metadata": {},
   "source": [
    "Hier könnte man jetzt eine neue Klasse definieren mit einem quadratischen Modell. Dafür muss man lediglich die Berechnung der Funktion und die Berechnung der Ableitung anpassen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c59007-4ea3-4edd-90e2-0e3abd746573",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: definiere Klasse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c411e73-2ca2-44e7-be78-a7f4ed49f28b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Linear_Regression_Bias(x,y)\n",
    "p_sol = gradientenverfahren(f=model, p_0=np.array([0,0]), T=10000, lr=0.00001)\n",
    "plt.scatter(x, y)\n",
    "plt.plot(x, model(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a6c3c3b-2b4b-4ce7-b536-3e096cc366ca",
   "metadata": {},
   "source": [
    "## Exponentiell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08845b98-5c9a-4d1c-ac3b-9b71152cb5c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_points = 100\n",
    "rand_scale = 10\n",
    "a = 1.1\n",
    "x_0 = np.array([i-50 for i in range(num_points)])\n",
    "x = x_0\n",
    "y = a**x_0+rand_scale*np.random.rand(num_points)-rand_scale/2\n",
    "\n",
    "plt.scatter(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77e5a4bb-402c-4add-be89-9744cfbc265a",
   "metadata": {},
   "source": [
    "Hier könnte man jetzt eine neue Klasse definieren mit einem exponentiellen Modell. Dafür muss man lediglich die Berechnung der Funktion und die Berechnung der Ableitung anpassen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deec467b-5df6-4c76-8c7c-06ca9a6f3916",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff501be2-5a6c-454a-81f2-29d82eda37d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Linear_Regression_Bias(x,y)\n",
    "p_sol = gradientenverfahren(f=model, p_0=np.array([0,0]), T=1000, lr=0.000001)\n",
    "plt.scatter(x, y)\n",
    "plt.plot(x, model(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9208ea5a-556c-4d1c-8ecd-29982e501b3f",
   "metadata": {},
   "source": [
    "# 4. Mehrdimensionale Lineare Regression\n",
    "Im Folgenden betrachten wir den Fall, dass $x$ nicht mehr eindimensional, sondern mehrdimensional ist, konkret am Beispiel von 3 Dimensionen (kann auch abgeändert werden)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e88c08d9-1299-4bbc-bfc3-3e74aa01a65b",
   "metadata": {},
   "source": [
    "## Datensatz\n",
    "Erst erstellen wir einen Mehrdimensionalen Datensatz. Einfach die folgende Zelle ausführen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd98d560-d6cc-4f68-ba79-2ca5f32162a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_points = 100\n",
    "dimensions = 3\n",
    "rand_scale = 0.2\n",
    "w = np.random.rand(dimensions)\n",
    "X = np.random.rand(num_points, dimensions) # X ist eine matrix also schreiben wir X ab jetzt groß\n",
    "y = np.matmul(X, w)#+rand_scale*np.random.rand(num_points)-rand_scale/2 # Der Fehler wurde hier fürs erste auskommentiert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba55fb2-e8e0-45c3-b7fd-6d4540e163f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(projection='3d')\n",
    "\n",
    "ax.scatter(X[:,0], X[:,1], y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ddf86c5-a073-4fc6-a593-8a5e72d76a37",
   "metadata": {},
   "source": [
    "3-dimensionale Daten zu visualisieren ist nicht einfach. Deshalb vergleichen wir ab jetzt nur noch unsere gefundenen Parameter $w_\\text{sol}$​ mit den wahren Parametern $w$. Man könnte auch den Verlust betrachten, also den quadratischen Fehler. Je kleiner, desto besser."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bd8a3a4-55c9-43ab-9ac3-540e383efdbd",
   "metadata": {},
   "source": [
    "Je nachdem, wie allgemein zuvor das Gradientenverfahren implementiert wurde, muss man das hier nicht neu implementieren. Aber falls doch, hier ist das Grundgerüst noch einmal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "642fc55d-4b5e-495e-b920-4458b88820a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    def gradientenverfahren(f, p_0, T, lr):\n",
    "        print(f\"Starting gradient descent with parameter p_0={p_0}, T={T}, and learning rate lr={lr}.\")\n",
    "        p = p_0\n",
    "        # TODO Berechne p T mal iterativ neu über den Gradientenabstieg, von f, mit Lernrate lr\n",
    "        \n",
    "        print(f\"Ended gradient descent with parameter p={p} and a gradient of {f.gradient(p)}.\")\n",
    "        return p"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1366fbf2-5a9a-4c0f-9a60-4e5a203b550d",
   "metadata": {},
   "source": [
    "Und zu guter Letzt muss wieder das Modell definiert werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b23d78-9db2-4128-87ab-f85704206c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear_Regression():\n",
    "    def __init__(self, X, y): # Hier speichern wir uns die Daten. Später kann man auf sie zurückgreifen mit self.X und self.y. Da X eine Matrix ist, X groß.\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "    \n",
    "    def __call__(self, w):\n",
    "        # TODO Wie zuvor berechne die Vorhersage. Diesmal ist er mehrdimensional und lässt sich mit Matrix multiplikation so beschreiben:\n",
    "        # Xw, mit X matrix, w Vektor\n",
    "        return self.y*0 \n",
    "\n",
    "    def gradient(self, w):\n",
    "        # TODO berechne den gradienten des quadratischen Fehlers in Bezug auf w. Also konkreter ||Xw-y||^2=||Xw||^2+||y||^2-2w^TX^Ty.\n",
    "        # Die Ableitung nach w (ein Vektor) ist dann 2X^TXw-2X^Ty = 2X^T(Xw-y).\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71989229-9fe0-4409-a193-fae6a4ed708a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Linear_Regression(X,y)\n",
    "w_sol = gradientenverfahren(f=model, p_0=np.zeros(dimensions), T=100, lr=0.01)\n",
    "\n",
    "print(f'The true parameter is {w} while the found parameter is {w_sol} thus the difference is {w-w_sol} which in metric terms is {np.linalg.norm(w-w_sol)} far away from the korrekt answer.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb00419-7e5e-44d7-8c45-a1f87e12362a",
   "metadata": {},
   "source": [
    "Das Interessante an der mehrdimensionalen Variante ist, dass man einen y-Achsenabschnitt sehr einfach integrieren kann. Konkret betrachten wir dann $f(X)=Xw+b=[X|1][w|b]$, hier heißt $[X|1]$, dass wir an die matrix $X$ einen Vektor voller 1en konkatenieren, und $[w|b]$ heißt, dass wir an $w$ $b$ konkatenieren. Somit müssen wir lediglich unser $w$ um ein Element verlängern und an alle Datenpunkte eine 1 anhängen. Das wird bereits in der folgenden Zelle durchgeführt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c58c81-8012-41fa-aa29-b45329b9544d",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = np.array([0.67])\n",
    "y = np.matmul(X, w)+b\n",
    "w_bias = np.concatenate([w, b])\n",
    "\n",
    "X = np.concatenate([X, np.ones(num_points)[:,None]], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc6d497a-1702-402d-a16a-ff7c62b9acfc",
   "metadata": {},
   "source": [
    "Dann folgt die gleiche Optimierung wie zuvor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed3a55e1-16b4-4fcf-807e-e50f09d4d91c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Linear_Regression(X,y)\n",
    "w_sol = gradientenverfahren(f=model, p_0=np.zeros(dimensions+1), T=1000, lr=0.001)\n",
    "print(f'Der wahre parameter ist {w_bias} und der gefundene parameter is {w_sol} somit ist der Unterschied {w_bias-w_sol} und in metrischen Termen sind wir {np.linalg.norm(w_bias-w_sol)} weit von der optimalen Lösung entfernt.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "535215ef-c855-44e7-a746-c2cd059934a8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 5 Neuronale Netze\n",
    "Zuletzt können wir dasselbe wie zuvor auch mit neuronalen Netzen durchführen. Neuronale Netze haben den Vorteil, dass sie jede Funktion darstellen können, unter ein paar Annahmen, zu denen wir gleich kommen.\n",
    "\n",
    "Ein neuronales Netz lässt sich beschreiben als $f(x)=w^T\\sigma(Wx+b_2)+b_1$, wobei die Eingabe $x$ $d$-dimensional ist und $w\\in\\mathbb R^h$, $W\\in\\mathbb R^{h \\times d}$, $b_2\\in\\mathbb R^h$, $b_1\\in \\mathbb R$. $\\sigma$ ist eine Aktivierungsfunktion und wir werden hier die Sigmoid-Funktion verwenden, also $\\sigma(x)=\\frac{1}{1+\\exp(-x)}$. $h$ ist hier die versteckte Dimension und je größer $h$ ist, desto \"komplexer\" kann die Funktion sein, die das neuronale Netz berechnen kann. Das hier beschriebene neuronale Netz hat $d$ Eingabeneuronen, $h$ versteckte Neuronen und 1 Ausgabeneuron. Das neuronale Netz hat 2 Schichten, konkret gibt es zwei lineare Operationen $w$ und $W$, welche Neuronen transformieren. $h$ entspricht also einer der Annahmen zuvor. Konkreter gesagt, gegeben eine Funktion $f$, eine beschränkte Domäne, auf der wir arbeiten, und einen Approximationsfaktor: Falls $h$ groß genug gewählt ist, kann das neuronale Netz $f$ in dieser Domäne dem Approximationsfaktor entsprechend approximieren.\n",
    "\n",
    "In der Realität werden auch viel tiefere neuronale Netze betrachtet, aber wir bleiben hier bei diesem einfachen Beispiel. Außerdem gibt es in der Realität auch andere Schichten. Die hier verwendeten Schichten werden typischerweise als vollständig verbundene Schicht (fully connected layer) beschrieben, und es gibt zum Beispiel auch Batchnorm-Schichten (batchnorm layer) oder Convolutional-Schichten (convolutional layer).\n",
    "\n",
    "Das hier beschriebene neuronale Netz ist ein Multilayer Perceptron (MLP) mit zwei Schichten, einer versteckten Dimension von $h$ und der Sigmoid-Aktivierungsfunktion.\n",
    "\n",
    "Die Berechnung der Gradienten der Parameter (in diesem Falle $w, W, b_2, b_1$) hängt stark von der Kettenregel ab und verläuft in einem Vorwärts- und Rückwärtsschritt. Konkret nennt man diese Methode Backpropagation, und sie verläuft in einem Vorwärtspass und einem Rückwärtspass. Weil das alles etwas zu kompliziert ist für die kurze Zeit, habe ich im Folgenden ein neuronales Netz definiert mit seiner Ableitung, damit ihr es ausprobieren könnt und eventuell sogar verstehen könnt, was hier abläuft."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e6d2d7a-6ddd-4c00-b9d8-da36034d92a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1.0/(1.0+(np.exp(-x)))\n",
    "\n",
    "def siggrad(x):\n",
    "    return x*(1-x)\n",
    "\n",
    "class MLP_Regression():\n",
    "    def __init__(self, X, y): # Hier speichern wir uns die Daten. Später kann man auf sie zurückgreifen mit self.X und self.y. Da X eine Matrix ist, X groß.\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "    \n",
    "    def __call__(self, p):\n",
    "        w, W, b2, b1 = p\n",
    "        return (np.matmul(w.T, sigmoid(np.matmul(W, self.X)+b2))+b1).T\n",
    "\n",
    "    def gradient(self, p):\n",
    "        w, W, b2, b1 = p\n",
    "        \n",
    "        # Forward pass. Hier werden die Komponenten für später vorberechnet\n",
    "        d  = sigmoid(np.matmul(W, self.X)+b2)\n",
    "        \n",
    "        o  = np.matmul(w.T, d)+b1-self.y\n",
    "        \n",
    "        # Backward pass. Hier werden die im Forward pass berechneten Komponenten für die gradienten benutzt.\n",
    "        wgrad  = 2*np.matmul(d, o.T)\n",
    "        b1grad = 2*np.sum(o) # Oder matmul mit 1\n",
    "        w_d = np.multiply(w ,siggrad(d)) # elementweise multiplikation\n",
    "        Wgrad  = 2*np.matmul(w_d, np.matmul(np.diag(o.flatten()), self.X.T)) # Das was hier zuletzt passiet ist eigentlich sehr komplex und nicht so einfach herleitbar. Konkret wird ein Vektor nach einer Matrix abgeleitet und wir arbeiten mit Tensoren. Das lässt sich aber alles wie hier beschrieben vereinfachen.\n",
    "        b2grad  = 2*np.matmul(w_d, o.T)\n",
    "        \n",
    "        return wgrad, Wgrad, b2grad, b1grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d91a8530-00e8-469e-ac80-0b2efba66649",
   "metadata": {},
   "source": [
    "Hier ist eine leicht abgeänderte Version des zuvor beschriebenen Gradientenabstiegs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16366ff1-9d09-4c04-a21e-e978fcfc7414",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradientenverfahrenmitmomentum(f, p_0, T, lr, beta=0.9):\n",
    "    print(f\"Starting gradient descent with momentum with parameter p_0={p_0}, T={T}, and learning rate lr={lr}.\")\n",
    "    p = p_0\n",
    "    m = tuple([0*v for v in p])\n",
    "    for t in range(T):\n",
    "        grad = f.gradient(p)\n",
    "        m = tuple([(beta*m[i])+((1-beta)*grad[i]) for i in range(len(m))])\n",
    "        p = tuple([p[i]-(lr*m[i]/(1-(beta**(t+1)))) for i in range(len(p))])\n",
    "    print(f\"Ended gradient descent with momentum with parameter p={p} and a gradient of {f.gradient(p)}.\")\n",
    "    return p"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88d1f279-d4fa-4afa-b89f-559cf37e94e3",
   "metadata": {},
   "source": [
    "Neuronale Netze sollen in der Lage sein, alle Funktionen zu approximieren. Das probieren wir im Folgenden aus. Als Funktion, die es zu approximieren gilt, verwenden wir zunächst die Sinusfunktion mit einer Frequenz, also $y(x)=\\sin(\\lambda\\pi x)$, in der Domäne $[0,1]$. Je höher wir $\\lambda$ wählen desto mehr oszilliert die Funktion und desto schwieriger ist es für das neuronale Netz, die Funktion zu approximieren.\n",
    "\n",
    "Da der folgende Prozess zufällig ist, kann es sein, dass ihr es mehrmals probieren müsst. Noch eine Anmerkung: Die Sigmoid-Aktivierungsfunktion ist eine eher alte Funktion und hat einen großen Nachteil. Ihre Ableitung wird sehr klein, wenn man sehr große (positive) oder kleine (große negative) Zahlen betrachtet. Also kann es sein, dass, wenn die Lernrate zu groß gewählt wird und der Optimierungsprozess divergiert, trotzdem ein sehr kleiner Gradient als Ergebnis produziert wird.\n",
    "\n",
    "Im folgenden kann es sein, dass ihr je nach Wahl die Anzahl an Schritten $T$ erhöhen müsst, was aber die Laufzeit deutlich beeinflusst."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f689ae75-04ba-4e61-bfe8-4f3baf95fe73",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO Ihr könnt gerne diese Funktion verändern um zu sehen was ein NN lernen kann. Erstmal würde ich aber empfehlen bei dieser Sinusfunktion zu bleiben.\n",
    "def target(x):\n",
    "    #TODO Fürs erste könt ihr hier lam erhöhen. Je höher lam gewählt wird umso höher muss auch h gewählt werden, aber es gibt keine 1:1 korrespondenz\n",
    "    lam = 2\n",
    "    return np.sin(lam*np.pi*x)\n",
    "\n",
    "num_points = 100\n",
    "X = np.expand_dims(np.array(np.arange(0, 1.0, 0.01)), axis=0) # TODO Ihr könnt die Datenhier auch gerne wieder zufällig ziehen, dann wird die Optimierung schwerer\n",
    "y = target(X)\n",
    "#TODO falls ihr mehr zufall wollt könnt ihr diese zwei Zeilen einkommentieren random_scale = 0.5\n",
    "# y += (np.random.rand(y.shape[0], y.shape[1])-0.5) * random_scale\n",
    "\n",
    "#TODO Dieses h hier könnt ihr verändern um auszuprobieren wieviel Komplexität das Neuronale Netz benötigt für eure gewählte Funktion.\n",
    "h = 3\n",
    "p = np.random.rand(h,1), np.random.rand(h,1), np.random.rand(h,1), np.random.rand(1) # Es ist wichtig zufällig (nicht mit 0) zu Initialisieren.\n",
    "\n",
    "p = gradientenverfahrenmitmomentum(MLP_Regression(X,y), p, 20000, 0.005)\n",
    "\n",
    "plt.scatter(X, y)\n",
    "_X = np.expand_dims(np.array(np.arange(-0.1, 1.1, 0.01)), axis=0)\n",
    "plt.plot(_X.T, target(_X.T), color='red', label='Ziel')\n",
    "plt.plot(_X.T, MLP_Regression(_X, y)(p), color='blue', label='Neuronales Netz')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a027ac57-2521-423e-96c7-3401e6e13629",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c78d3d-2dc7-4bb0-828a-0342bacb957a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
